# -*- coding: utf-8 -*-
"""QA_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b2xsb9C9jsXm1q39QG_e-HxwOKwesAAs
"""

!pip --version
!pip install pennylane

import pennylane as qml
qml.about()
from pennylane import numpy as numpy
import matplotlib.pyplot as plt
import math
import torch
from torch.autograd import Variable

dev = qml.device("default.qubit",wires=2)
gpu = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

import pennylane as qml
from pennylane import numpy as np
import math

print("================== start ==============")
# Circuit is hardcoded in several places to only have 2 qubits, despite the following variables.
# For instance, the feature_map() only takes in 1 value "phi",
# whereas multiple values would be needed for more qubits.
num_qubits = 2
shots = 10

L = 10

# This should be U_phi for 2 inputs
def U_phi(phi):
    qml.Hadamard(0) @ qml.Hadamard(1)
    qml.RZ(phi[0],wires=[0])
    qml.RZ(phi[1],wires=[1])
    qml.CNOT(wires=[0,1])
    qml.RZ(phi[2],wires=[1])
    qml.CNOT(wires=[0,1])

    qml.Hadamard(0) @ qml.Hadamard(1)
    qml.RZ(phi[0],wires=[0])
    qml.RZ(phi[1],wires=[1])
    qml.CNOT(wires=[0,1])
    qml.RZ(phi[2],wires=[1])
    qml.CNOT(wires=[0,1])


# this should be 1 part of W(theta)
# Should both rotations have the same parameters? or different?
# Figure seems to imply same params
def W_theta_part(param):
    qml.CZ(wires=[0,1])
    qml.Rot(param[0],param[1],param[2],wires=[0])
    qml.Rot(param[3],param[4],param[5],wires=[1])

# this should be the full circuit then
@qml.qnode(dev,interface='torch')
def circuit(input,params):
    U_phi(input)

    for param in params:
        W_theta_part(param)

    return [qml.expval(qml.PauliZ(0)),qml.expval(qml.PauliZ(1))]

def data_set_mapping(x,y):
    x = math.sin(x * math.pi * 8)
    y = math.cos(y * math.pi * 8)
    val = (x + y) / 2;
    return 1 if val > 0 else -1


DATA_SIZE = 100

X = np.random.random((DATA_SIZE,2))
Y = [data_set_mapping(i[0],i[1]) for i in X]
X = np.append(X,[[(math.pi - x[0])*(math.pi - x[1])] for x in X],axis=1)

type(circuit)
circuit.to(gpu)

X = torch.tensor(X)
X.cuda()
Y = torch.tensor(Y)
Y.cuda()

params = np.random.random((L,6)) * math.pi
params = Variable(torch.tensor(params),requires_grad=True)
params.cuda()

opt = torch.optim.Adam([params],lr = 0.1)

def loss(labels,predictions):
    loss = 0
    for l, p in zip(labels,predictions):
        loss = loss + (l - sum(p)) ** 2
    loss = loss / len(labels)
    return loss


def cost(params,X,Y):
    X = X * math.pi
    loss = Variable(torch.tensor(0))
    for x,y in zip(X,Y):
        pred = circuit(x,params)
        loss = loss + (y - torch.sum(pred)) ** 2
    loss = loss / len(Y)
    return loss

def closure():
    opt.zero_grad()
    loss = cost(params,X,Y)
    print(loss.item())
    loss.backward()
    return loss


print("start training")
for i in range(10):
    print("epoch: " + str(i))
    opt.step(closure)
